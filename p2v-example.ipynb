{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:17:26.624701Z",
     "start_time": "2019-04-17T02:17:25.755068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\r\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "!python3 setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:17:26.632448Z",
     "start_time": "2019-04-17T02:17:26.627317Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove console log and use file log \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s',level=logging.INFO)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s %(message)s')\n",
    "\n",
    "# filelogger = logging.getLogger()\n",
    "\n",
    "# for h in filelogger.handlers:\n",
    "#     filelogger.removeHandler(h)\n",
    "\n",
    "# filelogger.setLevel(logging.INFO)\n",
    "# fh = logging.FileHandler(\"log.log\",mode='w')\n",
    "# fh.setLevel(logging.DEBUG)\n",
    "# fh.setFormatter(formatter)\n",
    "# filelogger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:17:26.950081Z",
     "start_time": "2019-04-17T02:17:26.634356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang18f/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.cross_validation import cross_val_predict,train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "from sklearn.preprocessing import normalize,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:19:57.670358Z",
     "start_time": "2019-04-17T02:17:26.952326Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 22:17:27,010 initializing cython module\n",
      "2019-04-16 22:17:27,031 cython module initialized\n",
      "2019-04-16 22:17:27,034 use P2V\n",
      "2019-04-16 22:17:27,034 use TF-ICF\n",
      "2019-04-16 22:17:27,035 initilize the model\n",
      "2019-04-16 22:17:27,036 counting frequence\n",
      "2019-04-16 22:17:28,088 done\n",
      "2019-04-16 22:17:28,089 loading data\n",
      "2019-04-16 22:17:29,337 data contains 2708 papers, 25955 words\n",
      "2019-04-16 22:17:29,384 pre-processing negative sampling for w2v\n",
      "2019-04-16 22:17:29,394 pre-processing negative sampling for d2v\n",
      "2019-04-16 22:17:29,403 pre-processing negative sampling for n2v\n",
      "2019-04-16 22:17:29,405 init embeddings\n",
      "2019-04-16 22:17:29,418 done\n",
      "2019-04-16 22:17:29,420 building word draw table\n",
      "2019-04-16 22:17:29,421 building node draw table\n",
      "2019-04-16 22:17:29,422 starting trainig threads with 60000000 samples\n",
      "2019-04-16 22:17:32,112 progress: 1.67%, 1M samples trained, current loss 0.5798, current speed 0.37M/s, overall speed 0.37M/s, ETA: 158s\n",
      "2019-04-16 22:17:34,475 progress: 3.33%, 2M samples trained, current loss 0.4595, current speed 0.42M/s, overall speed 0.40M/s, ETA: 146s\n",
      "2019-04-16 22:17:37,242 progress: 5.00%, 3M samples trained, current loss 0.4317, current speed 0.36M/s, overall speed 0.38M/s, ETA: 148s\n",
      "2019-04-16 22:17:40,046 progress: 6.67%, 4M samples trained, current loss 0.4125, current speed 0.36M/s, overall speed 0.38M/s, ETA: 148s\n",
      "2019-04-16 22:17:42,614 progress: 8.33%, 5M samples trained, current loss 0.3986, current speed 0.39M/s, overall speed 0.38M/s, ETA: 145s\n",
      "2019-04-16 22:17:45,185 progress: 10.00%, 6M samples trained, current loss 0.3880, current speed 0.39M/s, overall speed 0.38M/s, ETA: 141s\n",
      "2019-04-16 22:17:47,441 progress: 11.67%, 7M samples trained, current loss 0.3800, current speed 0.44M/s, overall speed 0.39M/s, ETA: 136s\n",
      "2019-04-16 22:17:50,280 progress: 13.33%, 8M samples trained, current loss 0.3732, current speed 0.35M/s, overall speed 0.38M/s, ETA: 135s\n",
      "2019-04-16 22:17:53,070 progress: 15.00%, 9M samples trained, current loss 0.3681, current speed 0.36M/s, overall speed 0.38M/s, ETA: 134s\n",
      "2019-04-16 22:17:55,856 progress: 16.67%, 10M samples trained, current loss 0.3635, current speed 0.36M/s, overall speed 0.38M/s, ETA: 132s\n",
      "2019-04-16 22:17:58,519 progress: 18.33%, 11M samples trained, current loss 0.3597, current speed 0.38M/s, overall speed 0.38M/s, ETA: 129s\n",
      "2019-04-16 22:18:01,300 progress: 20.00%, 12M samples trained, current loss 0.3563, current speed 0.36M/s, overall speed 0.38M/s, ETA: 127s\n",
      "2019-04-16 22:18:04,107 progress: 21.67%, 13M samples trained, current loss 0.3537, current speed 0.36M/s, overall speed 0.37M/s, ETA: 125s\n",
      "2019-04-16 22:18:06,919 progress: 23.33%, 14M samples trained, current loss 0.3511, current speed 0.36M/s, overall speed 0.37M/s, ETA: 123s\n",
      "2019-04-16 22:18:09,286 progress: 25.00%, 15M samples trained, current loss 0.3493, current speed 0.42M/s, overall speed 0.38M/s, ETA: 119s\n",
      "2019-04-16 22:18:11,690 progress: 26.67%, 16M samples trained, current loss 0.3476, current speed 0.42M/s, overall speed 0.38M/s, ETA: 116s\n",
      "2019-04-16 22:18:14,062 progress: 28.33%, 17M samples trained, current loss 0.3455, current speed 0.42M/s, overall speed 0.38M/s, ETA: 112s\n",
      "2019-04-16 22:18:16,419 progress: 30.00%, 18M samples trained, current loss 0.3442, current speed 0.42M/s, overall speed 0.38M/s, ETA: 109s\n",
      "2019-04-16 22:18:18,777 progress: 31.67%, 19M samples trained, current loss 0.3427, current speed 0.42M/s, overall speed 0.38M/s, ETA: 106s\n",
      "2019-04-16 22:18:21,134 progress: 33.33%, 20M samples trained, current loss 0.3415, current speed 0.42M/s, overall speed 0.39M/s, ETA: 103s\n",
      "2019-04-16 22:18:23,492 progress: 35.00%, 21M samples trained, current loss 0.3405, current speed 0.42M/s, overall speed 0.39M/s, ETA: 100s\n",
      "2019-04-16 22:18:25,851 progress: 36.67%, 22M samples trained, current loss 0.3392, current speed 0.42M/s, overall speed 0.39M/s, ETA: 97s\n",
      "2019-04-16 22:18:28,210 progress: 38.33%, 23M samples trained, current loss 0.3385, current speed 0.42M/s, overall speed 0.39M/s, ETA: 94s\n",
      "2019-04-16 22:18:30,569 progress: 40.00%, 24M samples trained, current loss 0.3372, current speed 0.42M/s, overall speed 0.39M/s, ETA: 91s\n",
      "2019-04-16 22:18:32,928 progress: 41.67%, 25M samples trained, current loss 0.3362, current speed 0.42M/s, overall speed 0.39M/s, ETA: 88s\n",
      "2019-04-16 22:18:35,287 progress: 43.33%, 26M samples trained, current loss 0.3354, current speed 0.42M/s, overall speed 0.39M/s, ETA: 86s\n",
      "2019-04-16 22:18:37,647 progress: 45.00%, 27M samples trained, current loss 0.3345, current speed 0.42M/s, overall speed 0.40M/s, ETA: 83s\n",
      "2019-04-16 22:18:40,012 progress: 46.67%, 28M samples trained, current loss 0.3338, current speed 0.42M/s, overall speed 0.40M/s, ETA: 80s\n",
      "2019-04-16 22:18:42,382 progress: 48.33%, 29M samples trained, current loss 0.3329, current speed 0.42M/s, overall speed 0.40M/s, ETA: 77s\n",
      "2019-04-16 22:18:44,748 progress: 50.00%, 30M samples trained, current loss 0.3327, current speed 0.42M/s, overall speed 0.40M/s, ETA: 75s\n",
      "2019-04-16 22:18:47,108 progress: 51.67%, 31M samples trained, current loss 0.3316, current speed 0.42M/s, overall speed 0.40M/s, ETA: 72s\n",
      "2019-04-16 22:18:49,467 progress: 53.33%, 32M samples trained, current loss 0.3315, current speed 0.42M/s, overall speed 0.40M/s, ETA: 70s\n",
      "2019-04-16 22:18:51,826 progress: 55.00%, 33M samples trained, current loss 0.3306, current speed 0.42M/s, overall speed 0.40M/s, ETA: 67s\n",
      "2019-04-16 22:18:54,242 progress: 56.67%, 34M samples trained, current loss 0.3302, current speed 0.41M/s, overall speed 0.40M/s, ETA: 64s\n",
      "2019-04-16 22:18:56,679 progress: 58.33%, 35M samples trained, current loss 0.3294, current speed 0.41M/s, overall speed 0.40M/s, ETA: 62s\n",
      "2019-04-16 22:18:59,053 progress: 60.00%, 36M samples trained, current loss 0.3292, current speed 0.42M/s, overall speed 0.40M/s, ETA: 59s\n",
      "2019-04-16 22:19:01,415 progress: 61.67%, 37M samples trained, current loss 0.3282, current speed 0.42M/s, overall speed 0.40M/s, ETA: 57s\n",
      "2019-04-16 22:19:03,798 progress: 63.33%, 38M samples trained, current loss 0.3281, current speed 0.42M/s, overall speed 0.40M/s, ETA: 54s\n",
      "2019-04-16 22:19:06,158 progress: 65.00%, 39M samples trained, current loss 0.3276, current speed 0.42M/s, overall speed 0.40M/s, ETA: 52s\n",
      "2019-04-16 22:19:08,519 progress: 66.67%, 40M samples trained, current loss 0.3271, current speed 0.42M/s, overall speed 0.40M/s, ETA: 49s\n",
      "2019-04-16 22:19:10,896 progress: 68.33%, 41M samples trained, current loss 0.3266, current speed 0.42M/s, overall speed 0.40M/s, ETA: 47s\n",
      "2019-04-16 22:19:13,256 progress: 70.00%, 42M samples trained, current loss 0.3256, current speed 0.42M/s, overall speed 0.40M/s, ETA: 44s\n",
      "2019-04-16 22:19:15,616 progress: 71.67%, 43M samples trained, current loss 0.3257, current speed 0.42M/s, overall speed 0.40M/s, ETA: 41s\n",
      "2019-04-16 22:19:17,977 progress: 73.33%, 44M samples trained, current loss 0.3248, current speed 0.42M/s, overall speed 0.41M/s, ETA: 39s\n",
      "2019-04-16 22:19:20,336 progress: 75.00%, 45M samples trained, current loss 0.3251, current speed 0.42M/s, overall speed 0.41M/s, ETA: 36s\n",
      "2019-04-16 22:19:22,696 progress: 76.67%, 46M samples trained, current loss 0.3245, current speed 0.42M/s, overall speed 0.41M/s, ETA: 34s\n",
      "2019-04-16 22:19:25,054 progress: 78.33%, 47M samples trained, current loss 0.3241, current speed 0.42M/s, overall speed 0.41M/s, ETA: 31s\n",
      "2019-04-16 22:19:27,414 progress: 80.00%, 48M samples trained, current loss 0.3237, current speed 0.42M/s, overall speed 0.41M/s, ETA: 29s\n",
      "2019-04-16 22:19:29,773 progress: 81.67%, 49M samples trained, current loss 0.3228, current speed 0.42M/s, overall speed 0.41M/s, ETA: 27s\n",
      "2019-04-16 22:19:32,132 progress: 83.33%, 50M samples trained, current loss 0.3229, current speed 0.42M/s, overall speed 0.41M/s, ETA: 24s\n",
      "2019-04-16 22:19:34,491 progress: 85.00%, 51M samples trained, current loss 0.3228, current speed 0.42M/s, overall speed 0.41M/s, ETA: 22s\n",
      "2019-04-16 22:19:36,850 progress: 86.67%, 52M samples trained, current loss 0.3218, current speed 0.42M/s, overall speed 0.41M/s, ETA: 19s\n",
      "2019-04-16 22:19:39,213 progress: 88.33%, 53M samples trained, current loss 0.3220, current speed 0.42M/s, overall speed 0.41M/s, ETA: 17s\n",
      "2019-04-16 22:19:41,581 progress: 90.00%, 54M samples trained, current loss 0.3213, current speed 0.42M/s, overall speed 0.41M/s, ETA: 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 22:19:43,949 progress: 91.67%, 55M samples trained, current loss 0.3212, current speed 0.42M/s, overall speed 0.41M/s, ETA: 12s\n",
      "2019-04-16 22:19:46,316 progress: 93.33%, 56M samples trained, current loss 0.3211, current speed 0.42M/s, overall speed 0.41M/s, ETA: 9s\n",
      "2019-04-16 22:19:48,683 progress: 95.00%, 57M samples trained, current loss 0.3210, current speed 0.42M/s, overall speed 0.41M/s, ETA: 7s\n",
      "2019-04-16 22:19:51,084 progress: 96.67%, 58M samples trained, current loss 0.3208, current speed 0.42M/s, overall speed 0.41M/s, ETA: 4s\n",
      "2019-04-16 22:19:53,931 progress: 98.33%, 59M samples trained, current loss 0.3204, current speed 0.35M/s, overall speed 0.41M/s, ETA: 2s\n",
      "2019-04-16 22:19:57,372 progress: 100.00%, 60M samples trained, current loss 0.3200, current speed 0.29M/s, overall speed 0.41M/s, ETA: 0s\n",
      "2019-04-16 22:19:57,373 60000000 samples trained in 147 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "workers = 1\n",
    "alg = 'p2v_weighted'\n",
    "w2v_ratio = 1\n",
    "d2v_ratio = 1\n",
    "n2v_ratio = 0.2\n",
    "l2 = 1\n",
    "n2v_p = 0.7\n",
    "total_samples = 2e7\n",
    "word_window = 10\n",
    "output = './cora_enrich.emb'\n",
    "datapath = \"/home/zhang18f/datasets/Cora_enrich/\"\n",
    "\n",
    "\n",
    "from P2VDataIterator import DataIterator\n",
    "dataset = DataIterator(data_path=datapath)\n",
    "\n",
    "import model as paper2vec\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "if \"unweighted\" in alg:\n",
    "    def file_len(fname):\n",
    "        import subprocess\n",
    "        p = subprocess.Popen(['wc', '-w', fname], stdout=subprocess.PIPE,\n",
    "                                                  stderr=subprocess.PIPE)\n",
    "        result, err = p.communicate()\n",
    "        if p.returncode != 0:\n",
    "            raise IOError(err)\n",
    "        return int(result.strip().split()[0])\n",
    "    len_w = file_len(\"%s/texts.txt\" % datapath)\n",
    "    len_n = file_len(\"%s/links.txt\" % datapath)\n",
    "    w2v_ratio = word_window if w2v_ratio > 0 else 0\n",
    "    d2v_ratio = 1 if d2v_ratio > 0 else 0\n",
    "    n2v_ratio = len_n/len_w if n2v_ratio > 0 else 0\n",
    "    \n",
    "if \"p2v\" in alg.lower() and d2v_ratio <= 0 and n2v_ratio <= 0:\n",
    "    raise ValueError('p2v need d2v_ratio or n2v_ratio > 0')\n",
    "\n",
    "model = paper2vec.paper2vec(\n",
    "    dataset,\n",
    "    w2v_ratio = w2v_ratio,\n",
    "    d2v_ratio = d2v_ratio,\n",
    "    n2v_ratio = n2v_ratio,\n",
    "    workers = workers,\n",
    "    w2v_window = word_window,\n",
    "    alpha = 0.025,\n",
    "    min_alpha = 0.0001,\n",
    "    w2v_min_count = 0,\n",
    "    negative = 5,\n",
    "    noise_distribution = 0.75,\n",
    "    w2v_subsampling = 0,\n",
    "    d2v_subsampling = 0,\n",
    "    n2v_subsampling = 0,\n",
    "    n2v_p = n2v_p,\n",
    "    l2 = l2,\n",
    "    batch_size = int(1e6),\n",
    "    total_samples = total_samples,\n",
    "    shuffle = 0 if (\"pv_dbow\" in alg or \"sg\" in alg) and total_samples <= 5e8 else 1,\n",
    "    tfidf = 1 if \"tfidf\" in alg else 0,\n",
    "    LDE = 1 if \"LDE\" in alg else 0,\n",
    ")\n",
    "\n",
    "if alg == \"sg+pv_dbow\":\n",
    "    model.w2v_ratio = 1\n",
    "    model.d2v_ratio = 0\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples,\n",
    "    )\n",
    "    model.w2v_ratio = 0\n",
    "    model.d2v_ratio = 1\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples,\n",
    "    )\n",
    "elif alg == \"LDE_doc\":\n",
    "    model.w2v_ratio = 1\n",
    "    model.d2v_ratio = 0\n",
    "    model.n2v_ratio = 0\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples,\n",
    "    )\n",
    "elif alg == \"LDE\":\n",
    "    model.w2v_ratio = 1\n",
    "    model.d2v_ratio = 0\n",
    "    model.n2v_ratio = 1\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples * 2,\n",
    "    )\n",
    "elif alg == \"LDE_link\":\n",
    "    print(\"LDE_link\")\n",
    "    model.w2v_ratio = 0\n",
    "    model.d2v_ratio = 0\n",
    "    model.n2v_ratio = 1\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples,\n",
    "    )\n",
    "else:\n",
    "    ratio = 0\n",
    "    ratio += 1 if w2v_ratio > 0 else 0\n",
    "    ratio += 1 if d2v_ratio > 0 else 0\n",
    "    ratio += 1 if n2v_ratio > 0 else 0\n",
    "\n",
    "    model.train(\n",
    "        workers = workers,\n",
    "        report_delay = 1,\n",
    "        total_samples = total_samples * ratio,\n",
    "    )\n",
    "\n",
    "# save embeddings\n",
    "with open(output,'w') as f:\n",
    "    f.write(\"%s %s\\n\" % (model.paper_embeddings.shape[0],model.paper_embeddings.shape[1]))\n",
    "    for pid in range(model.paper_embeddings.shape[0]):\n",
    "        f.write(\"%s \" % model.id2paper[pid])\n",
    "        f.write(\" \".join([str(x) for x in model.paper_embeddings[pid]]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:19:58.077905Z",
     "start_time": "2019-04-17T02:19:57.672633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9000    0.8438    0.8710        32\n",
      "          1     0.8914    0.9231    0.9070       169\n",
      "          2     0.9057    0.8421    0.8727        57\n",
      "          3     0.9255    0.9775    0.9508        89\n",
      "          4     0.8082    0.8551    0.8310        69\n",
      "          5     0.9487    0.8810    0.9136        42\n",
      "          6     0.9744    0.9048    0.9383        84\n",
      "\n",
      "avg / total     0.9057    0.9041    0.9041       542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = open('%s/idxs.txt' % datapath)\n",
    "l = open('%s/labels.txt' % datapath)\n",
    "X = []\n",
    "Y = []\n",
    "labels = {}\n",
    "for idx,label in zip(i,l):\n",
    "    idx = idx.rstrip()\n",
    "    X.append(model.paper(idx))\n",
    "    label = label.rstrip()\n",
    "    if label not in labels:\n",
    "        labels[label] = len(labels)\n",
    "    Y.append(labels[label])\n",
    "    \n",
    "\n",
    "# X = normalize(X)\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,train_size=0.8,random_state=0)\n",
    "Y_predict = lg(random_state=0).fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,Y_predict,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T01:49:52.640802Z",
     "start_time": "2019-04-17T01:49:52.632575Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
